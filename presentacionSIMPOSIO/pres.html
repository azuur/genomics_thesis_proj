<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Assessment of Gene Regulatory Network Inference Algorithms Using Monte Carlo Simulations</title>
    <meta charset="utf-8" />
    <meta name="author" content="Adrián Zuur and Liliana López-Kleine" />
    <meta name="date" content="2019-07-19" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="azuur.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Assessment of Gene Regulatory Network Inference Algorithms Using Monte Carlo Simulations
### Adrián Zuur and Liliana López-Kleine
### Department of Statistics, UNAL-Bogotá
### July 19, 2019

---


class:left 





## What We Are Modeling

&lt;img src="image1.png" width="1500px" /&gt;


---

class:left 

## The Mathematical Model

.pull-left[

Gene regulatory networks (GRNs) are models that aim to encode the regulatory relations among genes in a genome.

- Genes are nodes, regulatory relations are edges.
- Regulatory relations are *causal* (edges are directed, indicate more than co-expression).
- Edges represent *direct* causal effects (indirect effects are directed paths).

GRNs are directed graphs `\((V,E)\)`, which are equivalent to an adjacency matrix.

]

.pull-right[
&lt;img src="image2.png" width="400px" /&gt;
]



---

class:left 

## The Data

.pull-left[
The typical kind of data to learn GRNs is **gene expression data**, which are direct or indirect counts of mRNA.
.center[
&lt;img src="image3.png" width="150px" /&gt;
]
Mostly, we have observational data, not experimental.



]

.pull-right[

&lt;img src="image4.png" width="800px" /&gt;
]


---

class:left 


## The Statistical Problem
In bioinformatics there are many methods to build GRNs. 


Papers introducing them typically test them on given datasets. They show that the methods work *in practice*.


Our question is, *do they work well in theory?*

- How dependent is a method on shape of regulatory relations, sample size, noise, etc.?

- How reliable is the method? Are reported results flukes?






---
class:left 


## Methods We Study
.Mid[
.shorties[
.pull-left[
##### Mutual information-based
.small[


.content-box-gray[
##### Mutual information network
Estimate

`$$I(X_i, X_j) = E\left(\log\frac{f_{X_i}(X_i)f_{X_j}(X_j) }{f_{X_iX_j}(X_i,X_j)}\right)$$`

Filter edges from MI matrix with threshold.
]

.content-box-gray[
##### ARACNe
For each triplet of variables, eliminate edge with lowest estimated MI.
]
.content-box-gray[
##### MRNET
Derive 'minimum redundancy, maximum relevance' score from estimated MI using a step-wise procedure. Threshold.
]
.content-box-gray[
##### CLR
Standardize estimated MI matrix row-wise and column-wise. Average both scores. Threshold.

]
]
]

.pull-right[
##### Regression-based
.small[

.content-box-gray[
##### NARROMI

For each gene, estimate LAD-Lasso. Use coefficients as scores for edges. Threshold.

$$
\hat{\beta} =\text{argmin}_{\beta \in \mathbb{R}^p} |Y-\beta^{\top}X|+\lambda||\beta||_1
$$
]

.content-box-gray[
##### TIGRESS
For each gene, estimate Least Angle Regression (LARS) repeatedly in a bootstrap-like procedure. Use estimates to compute scores of relevance in prediction. Threshold.
]

.content-box-gray[
##### GENIE3
For each gene, estimate an ensemble of regression tress (e.g. random forest). Use estimates to compute scores of relevance in prediction. Threshold.
]
]
]
]
]
---

class:left 

## The Probabilistic Model to Estimate
.Small[These inference algorithms are heuristics. Consider them as estimators and look at their statistical properties (unfair).

Estimators of what? **A Bayesian Network associated to a causal SEM**.
]
.pull-left[
#### Causal Structural Equations Model (SEM)

.center[
&lt;img src="image5.png" width="200px" /&gt;
]
.Small[
Each equation is a causal mechanism.

The joint distribution of noise variables `\(\epsilon_i\)` determines a joint distribution of gene expressions. This is `\(P_X(x)\)`.
]
]

.pull-right[

#### Bayesian Network

.center[
&lt;img src="image6.png" width="150px" /&gt;
]
.Small[
Assume joint independence of error variables (no unmeasured common causes), and aciclicity (no contemporary causation).

Draw an edge from direct causes to effects. This is a Bayesian Network, **our parameter of interest**.
]
]

---

class:left


## Side Note: On Causal Bayesian Networks


A Bayesian Network is related to the distribution `\(P_X(x)\)` by a graphical notion called **d-separation**.

d-separation implies conditional statistical independence.

$$
X\perp_dY|Z \implies X\perp Y|Z
$$

Under an arguably weak condition, **faithfulness**, the converse also holds:

$$
X\perp Y|Z \implies X\perp_dY|Z
$$

In this case, the skeleton of the BN (that is, the undirected network it induces naturally) is identified. Estimation is possible but computationally complex.





---

class:left 

## Workflow


.left-column[
.Small[
1. Extract 50 node sub-network from "gold standard" *S. cerevisiae* GRN.

2. Generate causal SEMs over this network.

3. For each causal SEM, simulate 1000 datasets of size 20, 50, 100, and 500. Apply algorithms on datasets.

4. Evaluate algorithm outputs.
]
]
.center[
&lt;img src="image7.png" width="460px" /&gt;

]






---

class:left 

## Sub-network

.left-column[

Sub-network extracted from "gold standard" GRN from Sisi Ma *et al.* (2014), using algorithm from Marbach *et al.* (2009). This algorithm maximizes modularity step-wise. Its output preserves some structural properties of its input.

]

.center[

&lt;img src="image8.png" width="550px" /&gt;


]


---

class:left


## Causal SEM Definition

Eight cases, fixed coefficients `\(\alpha_{ij} \sim U(-1,1)\)` throughout.


.pull-left[
* Functional form

`$$f_i\left(pa\left(X_i\right), \varepsilon_i \right)=\sum_{X_j \in pa(X_i)} \alpha_{ij}X_j + \varepsilon_i$$`


`$$f_i\left(pa\left(X_i\right),  \varepsilon_i\right)=2\sigma\left( \sum_{X_j \in pa(X_i)} \alpha_{ij} X_j \right)-1 + \varepsilon_i$$`



* Levels of noise

$$
FVU_i=\frac{Var(\varepsilon_i)}{Var(X_i)}=0.2
$$


$$
FVU_i=\frac{Var(\varepsilon_i)}{Var(X_i)}=0.8
$$

]


.pull-right[

* Distribution of error terms


$$
\varepsilon_{ij}\sim N(0,\sigma^2)
$$

$$
\varepsilon_{ij}\sim U(-b,b)
$$

]


---

class:left

## Estimation

.Small[
* Algorithms are used "out-of-the-box", that is, using parameters suggested by authors.

* Implementations by authors + reputable packages. NARROMI was translated from Matlab to R code.

* Mutual information was estimated with Miller-Madow estimator. Entropies and cross-entropies are estimated via maximum likelihood with discretized variables, plugged into definition of `\(I(X,Y)\)`, and a bias correction term is added.

`$$\hat{H(}X)= - \sum_{b_X \in \text{bins}_X}\hat{p}_{b_X}\log\left(\hat{p}_{b_X}\right)$$`
`$$\hat{H(}Y)= - \sum_{b_Y \in \text{bins}_Y}\hat{p}_{b_Y}\log\left(\hat{p}_{b_Y}\right)$$`

`$$\hat{H(X,Y)}= - \sum_{b_{X \times Y} \in \text{bins}_{X\times Y}}\hat{p}_{b_{X\times Y}}\log\left(\hat{p}_{b_{X\times Y}}\right)$$`

`$$\hat{I(}X,Y)= \hat{H(}X)+\hat{H(}Y)-\hat{H(}X,Y) + \frac{\hat{m}-1}{n}$$`

]

---

class:left 

## Assessment of Estimates

.pull-left[
.Small[
Each algorithm can be seen as a classifier that outputs scores `\(s_{ij}\)` for edges. For each threshold on scores we get

$$
\text{Sens.} = \frac{\text{# correctly detected edges} }{\text{# true edges}}
$$

$$
\text{Spec.} = \frac{\text{# correctly detected non-edges} }{\text{# true non-edges}}
$$
Over all thresholds, we get a parametric curve - the ROC curve. The area under it, AUROC, is the probability that a randomly sampled true edge has a score higher than that of a  randomly sampled non-edge.
]
]

.pull-right[

&lt;img src="image9.png" width="1000px" /&gt;

]

---

class:left

## Results

---

class:center

&lt;img src="image10.png" width="800px" /&gt;


---

class:center

&lt;img src="image11.gif" width="800px" /&gt;


---

class:left 


## Results

- MI-based algorithms are more variable than regression-based algorithms.

- MI-based algorithms are more sensitive to sample size than regression-based algorithms. ARACNe and NARROMI are the extremes.

- TIGRESS is most sensitive to `\(FVU\)`. ARACNe is least sensitive.

- Surprisingly good results for large `\(n\)`. Not so much for small `\(n\)`. 

- Better results with less noise, except at large sample size. Bias-variance trade-off.






---

class:left 

## Thanks

Thanks.

I'm interested in comments or suggestions.

My email is agzuurp@unal.edu.co, and we can talk outside.



---

class:left 

## References
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
